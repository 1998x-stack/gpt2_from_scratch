#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰‹å†™å­—èŠ‚çº§BPE Tokenizer

è¿™ä¸ªå®ç°å±•ç¤ºäº†ç°ä»£LLM tokenizerçš„æ ¸å¿ƒæœºåˆ¶ï¼š
1. Unicodeæ–‡æœ¬ -> UTF-8 bytesï¼ˆå­—èŠ‚åºåˆ—ï¼‰
2. Bytes -> é€šè¿‡BPEåˆå¹¶æˆtokens

ä¸ºä»€ä¹ˆç†è§£è¿™ä¸ªå¾ˆé‡è¦ï¼Ÿ
- å¾ˆå¤šæ¨¡å‹çš„"è«åå…¶å¦™çš„bug"å…¶å®æ˜¯tokenizeré—®é¢˜
- ç‰¹æ®Šå­—ç¬¦ã€å¤šè¯­è¨€æ–‡æœ¬ã€emojiçš„å¤„ç†éƒ½ä¾èµ–äºæ­£ç¡®çš„tokenizer
- ç†è§£tokenizationæœ‰åŠ©äºprompt engineeringå’Œdebug

ä½œè€…æ³¨ï¼šæœ¬å®ç°å‚è€ƒäº†GPT-2/GPT-4çš„å­—èŠ‚çº§BPEè®¾è®¡
"""

from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import re


class SimpleBPETokenizer:
    """
    ä¸€ä¸ªä»é›¶å®ç°çš„å­—èŠ‚çº§BPE Tokenizer
    
    æ ¸å¿ƒæ¦‚å¿µï¼š
    - å­—èŠ‚çº§(Byte-level)ï¼šç›´æ¥åœ¨UTF-8å­—èŠ‚ä¸Šæ“ä½œï¼Œè€Œéå­—ç¬¦
    - BPEï¼šè¿­ä»£åœ°åˆå¹¶æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹ï¼Œå½¢æˆæ›´é•¿çš„token
    
    ä¸ºä»€ä¹ˆç”¨å­—èŠ‚çº§ï¼Ÿ
    1. ä»»ä½•Unicodeå­—ç¬¦éƒ½èƒ½è¢«ç¼–ç ï¼ˆä¸ä¼šæœ‰OOVé—®é¢˜ï¼‰
    2. è¯è¡¨å¤§å°å¯æ§ï¼ˆåŸºç¡€è¯è¡¨åªæœ‰256ä¸ªå­—èŠ‚ï¼‰
    3. GPTç³»åˆ—æ¨¡å‹éƒ½ç”¨è¿™ç§æ–¹å¼
    """
    
    def __init__(self, vocab_size: int = 500):
        """
        åˆå§‹åŒ–tokenizer
        
        Args:
            vocab_size: ç›®æ ‡è¯è¡¨å¤§å°ï¼ŒåŒ…å«256ä¸ªåŸºç¡€å­—èŠ‚token
                        å®é™…ä¼šè¿›è¡Œ vocab_size - 256 æ¬¡åˆå¹¶æ“ä½œ
        """
        self.vocab_size = vocab_size
        
        # è¯è¡¨ï¼štoken_id -> bytes
        # åˆå§‹åŒ–ä¸º256ä¸ªåŸºç¡€å­—èŠ‚ (0x00 - 0xFF)
        self.vocab: Dict[int, bytes] = {i: bytes([i]) for i in range(256)}
        
        # åˆå¹¶è§„åˆ™ï¼š(byte1, byte2) -> merged_token_id
        # è¿™ä¸ªé¡ºåºå¾ˆé‡è¦ï¼ç¼–ç æ—¶å¿…é¡»æŒ‰è®­ç»ƒæ—¶çš„é¡ºåºåº”ç”¨åˆå¹¶
        self.merges: Dict[Tuple[int, int], int] = {}
        
        # ç‰¹æ®Štokenï¼ˆå¯æ‰©å±•ï¼‰
        self.special_tokens: Dict[str, int] = {}
        
        # ç”¨äºåˆ†è¯çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆç±»ä¼¼GPT-2çš„é¢„åˆ†è¯ï¼‰
        # è¿™ä¸ªæ­£åˆ™æŠŠæ–‡æœ¬åˆ‡æˆæ›´å°çš„å—ï¼Œé¿å…è·¨è¯åˆå¹¶
        self.pat = re.compile(
            r"""'s|'t|'re|'ve|'m|'ll|'d| ?\w+| ?\d+| ?[^\s\w\d]+|\s+(?!\S)|\s+""",
            re.UNICODE
        )
    
    # ==================== Unicodeä¸Bytesçš„å…³ç³» ====================
    
    def _explain_unicode_to_bytes(self, text: str) -> None:
        """
        ã€æ•™å­¦æ–¹æ³•ã€‘è§£é‡ŠUnicodeæ–‡æœ¬å¦‚ä½•å˜æˆUTF-8å­—èŠ‚
        
        UTF-8ç¼–ç è§„åˆ™ï¼š
        - 1å­—èŠ‚: 0xxxxxxx (ASCII, 0-127)
        - 2å­—èŠ‚: 110xxxxx 10xxxxxx (æ‹‰ä¸æ‰©å±•ã€å¸Œè…Šç­‰)
        - 3å­—èŠ‚: 1110xxxx 10xxxxxx 10xxxxxx (ä¸­æ—¥éŸ©ã€å¤§å¤šæ•°è¯­è¨€)
        - 4å­—èŠ‚: 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx (emojiã€å¤æ–‡å­—)
        
        è¿™å°±æ˜¯ä¸ºä»€ä¹ˆï¼š
        - è‹±æ–‡tokené€šå¸¸çŸ­ï¼Œä¸­æ–‡tokené€šå¸¸é•¿
        - åŒæ ·çš„vocab_sizeï¼Œè‹±æ–‡èƒ½è¡¨ç¤ºæ›´å¤š"æ¦‚å¿µ"
        """
        print(f"\n{'='*60}")
        print(f"Unicode -> UTF-8 Bytes è§£æ: '{text}'")
        print(f"{'='*60}")
        
        for char in text:
            code_point = ord(char)  # Unicodeç ç‚¹
            utf8_bytes = char.encode('utf-8')  # UTF-8å­—èŠ‚
            
            # åˆ¤æ–­å­—èŠ‚æ•°
            if code_point < 0x80:
                byte_type = "1å­—èŠ‚ (ASCII)"
            elif code_point < 0x800:
                byte_type = "2å­—èŠ‚"
            elif code_point < 0x10000:
                byte_type = "3å­—èŠ‚"
            else:
                byte_type = "4å­—èŠ‚ (emoji/ç¨€æœ‰)"
            
            print(f"  '{char}' | U+{code_point:04X} | {byte_type}")
            print(f"       UTF-8: {list(utf8_bytes)} -> {[hex(b) for b in utf8_bytes]}")
    
    # ==================== BPE è®­ç»ƒ ====================
    
    def _get_stats(self, token_ids_list: List[List[int]]) -> Dict[Tuple[int, int], int]:
        """
        ç»Ÿè®¡æ‰€æœ‰ç›¸é‚»tokenå¯¹çš„å‡ºç°é¢‘ç‡
        
        è¿™æ˜¯BPEçš„æ ¸å¿ƒï¼šæ‰¾åˆ°æœ€é¢‘ç¹çš„ç›¸é‚»å¯¹
        
        Args:
            token_ids_list: å¤šä¸ªtokenåºåˆ—çš„åˆ—è¡¨
        
        Returns:
            {(token1, token2): count} çš„å­—å…¸
        """
        stats = defaultdict(int)
        for token_ids in token_ids_list:
            for i in range(len(token_ids) - 1):
                pair = (token_ids[i], token_ids[i + 1])
                stats[pair] += 1
        return stats
    
    def _merge(self, token_ids: List[int], pair: Tuple[int, int], new_id: int) -> List[int]:
        """
        åœ¨tokenåºåˆ—ä¸­æ‰§è¡Œä¸€æ¬¡åˆå¹¶æ“ä½œ
        
        ä¾‹å¦‚: [1, 2, 3, 2, 3] + merge(2,3)->99 = [1, 99, 99]
        
        Args:
            token_ids: åŸå§‹tokenåºåˆ—
            pair: è¦åˆå¹¶çš„tokenå¯¹ (a, b)
            new_id: åˆå¹¶åçš„æ–°token id
        
        Returns:
            åˆå¹¶åçš„æ–°åºåˆ—
        """
        new_tokens = []
        i = 0
        while i < len(token_ids):
            # æ£€æŸ¥å½“å‰ä½ç½®æ˜¯å¦åŒ¹é…è¦åˆå¹¶çš„pair
            if (i < len(token_ids) - 1 and 
                token_ids[i] == pair[0] and 
                token_ids[i + 1] == pair[1]):
                new_tokens.append(new_id)
                i += 2  # è·³è¿‡ä¸¤ä¸ªtoken
            else:
                new_tokens.append(token_ids[i])
                i += 1
        return new_tokens
    
    def train(self, texts: List[str], verbose: bool = True) -> None:
        """
        åœ¨ç»™å®šæ–‡æœ¬ä¸Šè®­ç»ƒBPEæ¨¡å‹
        
        BPEè®­ç»ƒç®—æ³•ï¼š
        1. å°†æ‰€æœ‰æ–‡æœ¬è½¬æ¢ä¸ºUTF-8å­—èŠ‚åºåˆ—
        2. ç»Ÿè®¡æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„é¢‘ç‡
        3. åˆå¹¶æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹ï¼Œç”Ÿæˆæ–°token
        4. é‡å¤æ­¥éª¤2-3ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡vocab_size
        
        Args:
            texts: è®­ç»ƒæ–‡æœ¬åˆ—è¡¨
            verbose: æ˜¯å¦æ‰“å°è®­ç»ƒè¿‡ç¨‹
        """
        if verbose:
            print("\n" + "="*60)
            print("å¼€å§‹BPEè®­ç»ƒ")
            print("="*60)
        
        # Step 1: é¢„åˆ†è¯ + è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—
        # é¢„åˆ†è¯çš„ä½œç”¨ï¼šé¿å…è·¨è¯è¾¹ç•Œçš„åˆå¹¶ï¼ˆå¦‚ "dog" + " cat" ä¸åº”åˆå¹¶ï¼‰
        token_ids_list: List[List[int]] = []
        
        for text in texts:
            # ä½¿ç”¨æ­£åˆ™é¢„åˆ†è¯
            chunks = self.pat.findall(text)
            for chunk in chunks:
                # æ¯ä¸ªchunkè½¬æ¢ä¸ºUTF-8å­—èŠ‚åºåˆ—
                # æ­¤æ—¶æ¯ä¸ªå­—èŠ‚å°±æ˜¯ä¸€ä¸ªtoken (0-255)
                utf8_bytes = chunk.encode('utf-8')
                token_ids_list.append(list(utf8_bytes))
        
        if verbose:
            total_tokens = sum(len(ids) for ids in token_ids_list)
            print(f"é¢„åˆ†è¯åå…±æœ‰ {len(token_ids_list)} ä¸ªchunk")
            print(f"åˆå§‹tokenæ•°é‡: {total_tokens}")
        
        # Step 2-4: è¿­ä»£åˆå¹¶
        num_merges = self.vocab_size - 256  # éœ€è¦è¿›è¡Œçš„åˆå¹¶æ¬¡æ•°
        
        for i in range(num_merges):
            # ç»Ÿè®¡å½“å‰æ‰€æœ‰ç›¸é‚»å¯¹çš„é¢‘ç‡
            stats = self._get_stats(token_ids_list)
            
            if not stats:
                if verbose:
                    print(f"æ²¡æœ‰æ›´å¤šå¯åˆå¹¶çš„pairï¼Œåœæ­¢äº {i} æ¬¡åˆå¹¶")
                break
            
            # æ‰¾åˆ°æœ€é¢‘ç¹çš„pair
            best_pair = max(stats, key=stats.get)
            best_count = stats[best_pair]
            
            # åˆ†é…æ–°çš„token id
            new_id = 256 + i
            
            # æ›´æ–°è¯è¡¨å’Œåˆå¹¶è§„åˆ™
            self.vocab[new_id] = self.vocab[best_pair[0]] + self.vocab[best_pair[1]]
            self.merges[best_pair] = new_id
            
            # åœ¨æ‰€æœ‰åºåˆ—ä¸­æ‰§è¡Œè¿™æ¬¡åˆå¹¶
            token_ids_list = [self._merge(ids, best_pair, new_id) for ids in token_ids_list]
            
            if verbose and (i < 10 or i % 50 == 0):
                # å°è¯•è§£ç ä»¥æ˜¾ç¤ºåˆå¹¶çš„æ˜¯ä»€ä¹ˆ
                try:
                    decoded = self.vocab[new_id].decode('utf-8', errors='replace')
                except:
                    decoded = repr(self.vocab[new_id])
                print(f"åˆå¹¶ #{i+1}: {best_pair} -> {new_id} "
                      f"(å‡ºç°{best_count}æ¬¡) = '{decoded}'")
        
        if verbose:
            final_tokens = sum(len(ids) for ids in token_ids_list)
            print(f"\nè®­ç»ƒå®Œæˆï¼è¯è¡¨å¤§å°: {len(self.vocab)}")
            print(f"Tokenæ•°é‡: {total_tokens} -> {final_tokens} "
                  f"(å‹ç¼©ç‡: {total_tokens/final_tokens:.2f}x)")
    
    # ==================== ç¼–ç ï¼ˆæ–‡æœ¬ -> Token IDsï¼‰ ====================
    
    def encode(self, text: str, verbose: bool = False) -> List[int]:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºtoken idåºåˆ—
        
        ç¼–ç è¿‡ç¨‹ï¼š
        1. é¢„åˆ†è¯ï¼ˆå¯é€‰ï¼Œç”¨æ­£åˆ™åˆ‡åˆ†ï¼‰
        2. è½¬æ¢ä¸ºUTF-8å­—èŠ‚
        3. æŒ‰è®­ç»ƒæ—¶çš„é¡ºåºåº”ç”¨æ‰€æœ‰åˆå¹¶è§„åˆ™
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            verbose: æ˜¯å¦æ‰“å°ç¼–ç è¿‡ç¨‹
        
        Returns:
            token idåˆ—è¡¨
        """
        if not text:
            return []
        
        # é¢„åˆ†è¯
        chunks = self.pat.findall(text)
        all_token_ids = []
        
        for chunk in chunks:
            # è½¬æ¢ä¸ºå­—èŠ‚ï¼ˆåˆå§‹tokenï¼‰
            token_ids = list(chunk.encode('utf-8'))
            
            if verbose:
                print(f"\nChunk: '{chunk}'")
                print(f"  UTF-8 bytes: {token_ids}")
            
            # æŒ‰åˆå¹¶é¡ºåºåº”ç”¨è§„åˆ™
            # å…³é”®ç‚¹ï¼šå¿…é¡»æŒ‰è®­ç»ƒæ—¶çš„é¡ºåºï¼
            # è¿™å°±æ˜¯ä¸ºä»€ä¹ˆmergesè¦è®°å½•é¡ºåº
            for pair, new_id in self.merges.items():
                token_ids = self._merge(token_ids, pair, new_id)
            
            if verbose:
                print(f"  åˆå¹¶å: {token_ids}")
            
            all_token_ids.extend(token_ids)
        
        return all_token_ids
    
    # ==================== è§£ç ï¼ˆToken IDs -> æ–‡æœ¬ï¼‰ ====================
    
    def decode(self, token_ids: List[int]) -> str:
        """
        å°†token idåºåˆ—è§£ç ä¸ºæ–‡æœ¬
        
        è§£ç è¿‡ç¨‹ï¼š
        1. æ¯ä¸ªtoken idæŸ¥è¡¨å¾—åˆ°bytes
        2. æ‹¼æ¥æ‰€æœ‰bytes
        3. ç”¨UTF-8è§£ç ä¸ºå­—ç¬¦ä¸²
        
        è¿™é‡Œæœ‰ä¸ªå‘ï¼š
        - å¦‚æœtokenè¢«åˆ‡æ–­åœ¨UTF-8åºåˆ—ä¸­é—´ï¼Œè§£ç ä¼šå‡ºé”™
        - è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæœ‰äº›æ¨¡å‹åœ¨å¤„ç†å¤šè¯­è¨€æ—¶ä¼šå‡ºbug
        
        Args:
            token_ids: token idåˆ—è¡¨
        
        Returns:
            è§£ç åçš„æ–‡æœ¬
        """
        # æ‹¼æ¥æ‰€æœ‰bytes
        all_bytes = b''.join(self.vocab.get(id, b'') for id in token_ids)
        
        # UTF-8è§£ç ï¼ˆerrors='replace'å¤„ç†æ— æ•ˆåºåˆ—ï¼‰
        return all_bytes.decode('utf-8', errors='replace')
    
    def decode_tokens(self, token_ids: List[int]) -> List[str]:
        """
        å°†æ¯ä¸ªtokenå•ç‹¬è§£ç ï¼Œç”¨äºè°ƒè¯•
        
        è¿™ä¸ªæ–¹æ³•èƒ½å¸®ä½ çœ‹æ¸…æ¯ä¸ªtokenä»£è¡¨ä»€ä¹ˆ
        """
        result = []
        for id in token_ids:
            token_bytes = self.vocab.get(id, b'')
            try:
                decoded = token_bytes.decode('utf-8')
            except UnicodeDecodeError:
                # UTF-8ä¸å®Œæ•´ï¼Œæ˜¾ç¤ºåŸå§‹å­—èŠ‚
                decoded = repr(token_bytes)
            result.append(decoded)
        return result
    
    # ==================== è°ƒè¯•å·¥å…· ====================
    
    def analyze_tokenization(self, text: str) -> None:
        """
        æ·±å…¥åˆ†æä¸€æ®µæ–‡æœ¬çš„tokenizationè¿‡ç¨‹
        
        è¿™ä¸ªæ–¹æ³•èƒ½å¸®ä½ ç†è§£ï¼š
        - ä¸ºä»€ä¹ˆæŸäº›æ–‡æœ¬tokenæ•°ç‰¹åˆ«å¤š
        - ä¸ºä»€ä¹ˆæ¨¡å‹åœ¨æŸäº›å­—ç¬¦ä¸Šè¡¨ç°å¥‡æ€ª
        """
        print("\n" + "="*60)
        print(f"Tokenizationåˆ†æ: '{text[:50]}{'...' if len(text) > 50 else ''}'")
        print("="*60)
        
        # å…ˆå±•ç¤ºUnicodeåˆ°bytesçš„è½¬æ¢
        self._explain_unicode_to_bytes(text[:20] if len(text) > 20 else text)
        
        # ç¼–ç 
        token_ids = self.encode(text)
        decoded_tokens = self.decode_tokens(token_ids)
        
        print(f"\næ€»Tokenæ•°: {len(token_ids)}")
        print(f"å­—ç¬¦æ•°: {len(text)}")
        print(f"Token/å­—ç¬¦æ¯”: {len(token_ids)/len(text):.2f}")
        
        print(f"\nTokenè¯¦æƒ…:")
        for i, (id, token) in enumerate(zip(token_ids, decoded_tokens)):
            # æ˜¾ç¤ºåŸå§‹bytes
            token_bytes = self.vocab.get(id, b'')
            print(f"  [{i}] id={id:4d} | '{token}' | bytes={list(token_bytes)}")
        
        # éªŒè¯è§£ç 
        decoded = self.decode(token_ids)
        if decoded == text:
            print("\nâœ“ ç¼–è§£ç éªŒè¯é€šè¿‡")
        else:
            print(f"\nâœ— ç¼–è§£ç ä¸åŒ¹é…!")
            print(f"  åŸæ–‡: {repr(text)}")
            print(f"  è§£ç : {repr(decoded)}")


# ==================== ç‰¹æ®Šæƒ…å†µæ¼”ç¤º ====================

def demonstrate_tokenization_issues():
    """
    æ¼”ç¤ºå¸¸è§çš„tokenizationé—®é¢˜
    
    è¿™äº›é—®é¢˜å°±æ˜¯å¾ˆå¤š"æ¨¡å‹bug"çš„çœŸæ­£åŸå› 
    """
    print("\n" + "="*70)
    print("å¸¸è§Tokenizationé—®é¢˜æ¼”ç¤º")
    print("="*70)
    
    # é—®é¢˜1: ä¸­è‹±æ–‡tokené•¿åº¦å·®å¼‚
    print("\nã€é—®é¢˜1ã€‘ä¸­è‹±æ–‡Tokené•¿åº¦å·®å¼‚")
    print("-"*40)
    en_text = "hello world"
    zh_text = "ä½ å¥½ä¸–ç•Œ"
    
    print(f"è‹±æ–‡ '{en_text}':")
    print(f"  å­—ç¬¦æ•°: {len(en_text)}")
    print(f"  UTF-8å­—èŠ‚æ•°: {len(en_text.encode('utf-8'))}")
    
    print(f"ä¸­æ–‡ '{zh_text}':")
    print(f"  å­—ç¬¦æ•°: {len(zh_text)}")
    print(f"  UTF-8å­—èŠ‚æ•°: {len(zh_text.encode('utf-8'))}")
    print("  ç»“è®º: åŒæ ·4ä¸ªå­—ç¬¦ï¼Œä¸­æ–‡éœ€è¦3å€çš„å­—èŠ‚ï¼Œæ„å‘³ç€æ›´å¤šçš„token")
    
    # é—®é¢˜2: Emojiå’Œç‰¹æ®Šå­—ç¬¦
    print("\nã€é—®é¢˜2ã€‘Emojiå’Œç‰¹æ®Šå­—ç¬¦")
    print("-"*40)
    emoji_text = "ğŸ˜€ğŸ‰"
    print(f"Emoji '{emoji_text}':")
    for emoji in emoji_text:
        utf8 = emoji.encode('utf-8')
        print(f"  '{emoji}' = {len(utf8)}å­—èŠ‚ = {list(utf8)}")
    print("  ç»“è®º: æ¯ä¸ªemojiéœ€è¦4å­—èŠ‚ï¼Œå¯èƒ½éœ€è¦å¤šä¸ªtokenè¡¨ç¤º")
    
    # é—®é¢˜3: ç©ºæ ¼å’Œæ¢è¡Œçš„è¯¡å¼‚è¡Œä¸º
    print("\nã€é—®é¢˜3ã€‘ç©ºæ ¼å’Œæ¢è¡Œçš„tokenåŒ–")
    print("-"*40)
    texts = ["hello", " hello", "  hello", "hello\n", "hello\t"]
    for t in texts:
        print(f"  {repr(t):15s} -> bytes: {list(t.encode('utf-8'))}")
    print("  ç»“è®º: å‰å¯¼ç©ºæ ¼ã€å¤šä¸ªç©ºæ ¼ã€æ¢è¡Œç¬¦éƒ½æ˜¯ç‹¬ç«‹tokenï¼Œå½±å“æ¨¡å‹ç†è§£")
    
    # é—®é¢˜4: æ•°å­—çš„åˆ‡åˆ†
    print("\nã€é—®é¢˜4ã€‘æ•°å­—çš„tokenåŒ–")
    print("-"*40)
    numbers = ["123", "1234", "12345", "123456789"]
    print("  æ•°å­—å¯èƒ½è¢«åˆ‡æˆå¥‡æ€ªçš„ç»„åˆ:")
    for n in numbers:
        utf8 = list(n.encode('utf-8'))
        print(f"  '{n}' -> bytes: {utf8}")
    print("  ç»“è®º: å¤§æ•°å­—å¯èƒ½è¢«åˆ‡æˆå¤šä¸ªtokenï¼Œå½±å“æ•°å­¦æ¨ç†")


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå®Œæ•´çš„tokenizerå·¥ä½œæµç¨‹"""
    
    # è®­ç»ƒæ•°æ®ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ç”¨æ›´å¤§çš„è¯­æ–™ï¼‰
    training_texts = [
        "Hello world! This is a simple BPE tokenizer.",
        "The quick brown fox jumps over the lazy dog.",
        "Machine learning and natural language processing are fascinating.",
        "ä½ å¥½ä¸–ç•Œï¼è¿™æ˜¯ä¸€ä¸ªç®€å•çš„BPEåˆ†è¯å™¨ã€‚",
        "æ·±åº¦å­¦ä¹ æ”¹å˜äº†äººå·¥æ™ºèƒ½çš„å‘å±•æ–¹å‘ã€‚",
        "Python is a great programming language for AI development.",
        "Tokenization is crucial for understanding language models.",
        "Special characters like @#$% need careful handling.",
        "Numbers like 12345 and dates like 2024-01-15 are tricky.",
        "Emojis ğŸ˜€ğŸ‰ are encoded as multiple bytes in UTF-8.",
    ]
    
    # åˆ›å»ºå¹¶è®­ç»ƒtokenizer
    tokenizer = SimpleBPETokenizer(vocab_size=350)  # 256 + 94æ¬¡åˆå¹¶
    tokenizer.train(training_texts, verbose=True)
    
    # æµ‹è¯•ç¼–è§£ç 
    print("\n" + "="*60)
    print("æµ‹è¯•ç¼–è§£ç ")
    print("="*60)
    
    test_texts = [
        "Hello world!",
        "ä½ å¥½ä¸–ç•Œï¼",
        "Machine learning is cool ğŸ˜€",
        "Test 12345 numbers",
    ]
    
    for text in test_texts:
        tokens = tokenizer.encode(text)
        decoded = tokenizer.decode(tokens)
        print(f"\nåŸæ–‡: '{text}'")
        print(f"Token IDs: {tokens}")
        print(f"Tokenæ•°: {len(tokens)}")
        print(f"è§£ç : '{decoded}'")
        print(f"åŒ¹é…: {'âœ“' if text == decoded else 'âœ—'}")
    
    # è¯¦ç»†åˆ†æ
    tokenizer.analyze_tokenization("Hello ä½ å¥½ ğŸ˜€")
    
    # æ¼”ç¤ºå¸¸è§é—®é¢˜
    demonstrate_tokenization_issues()
    
    print("\n" + "="*60)
    print("æ€»ç»“ï¼šä¸ºä»€ä¹ˆtokenizeré—®é¢˜ä¼šå¯¼è‡´æ¨¡å‹bugï¼Ÿ")
    print("="*60)
    print("""
1. ã€ä¸Šä¸‹æ–‡çª—å£é—®é¢˜ã€‘
   - ä¸­æ–‡æ¯å­—ç¬¦å 3å­—èŠ‚ï¼Œè‹±æ–‡åªå 1å­—èŠ‚
   - åŒæ ·çš„tokené™åˆ¶ï¼Œä¸­æ–‡èƒ½å®¹çº³çš„å†…å®¹æ›´å°‘
   
2. ã€æ•°å­¦æ¨ç†å›°éš¾ã€‘
   - æ•°å­—è¢«æ‹†åˆ†æˆå¥‡æ€ªçš„tokenç»„åˆ
   - æ¨¡å‹éš¾ä»¥ç†è§£123å’Œ12ã€3çš„å…³ç³»

3. ã€ç‰¹æ®Šå­—ç¬¦ä¹±ç ã€‘
   - emojiã€ç½•è§å­—ç¬¦å¯èƒ½è¢«é”™è¯¯åˆ‡åˆ†
   - å¯¼è‡´ç”Ÿæˆæ—¶å‡ºç°ä¹±ç æˆ–æˆªæ–­

4. ã€ç©ºæ ¼æ•æ„Ÿæ€§ã€‘
   - "hello"å’Œ" hello"æ˜¯å®Œå…¨ä¸åŒçš„tokenåºåˆ—
   - å¯¼è‡´promptå¾®å°å˜åŒ–äº§ç”Ÿä¸åŒç»“æœ

5. ã€å¤šè¯­è¨€ä¸å…¬å¹³ã€‘
   - æŸäº›è¯­è¨€éœ€è¦æ›´å¤štokenè¡¨ç¤ºç›¸åŒå«ä¹‰
   - æ¨¡å‹åœ¨è¿™äº›è¯­è¨€ä¸Šæ€§èƒ½ä¼šä¸‹é™

ç†è§£è¿™äº›ï¼Œå°±èƒ½æ›´å¥½åœ°debugæ¨¡å‹è¡Œä¸ºï¼
""")


if __name__ == "__main__":
    main()
